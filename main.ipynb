{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    images_int = (images * 255).astype(\"uint8\")\n",
    "    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n",
    "    return round(float(clip_score), 4)\n",
    "\n",
    "sd_clip_score = calculate_clip_score(images, prompts)\n",
    "print(f\"CLIP score: {sd_clip_score}\")\n",
    "# CLIP score: 35.7038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: 0.7782825827598572\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# Load and preprocess the two images\n",
    "image_a_path = \"D:\\Code\\py\\evaluator_pipeline\\in.jpg\"  # Replace with the path to your first image\n",
    "image_b_path = \"D:\\Code\\py\\evaluator_pipeline\\out_lineart+canny.png\"  # Replace with the path to your second image\n",
    "image_a_inputs = preprocess_image(image_a_path)\n",
    "image_b_inputs = preprocess_image(image_b_path)\n",
    "\n",
    "# Calculate the image embeddings\n",
    "with torch.no_grad():\n",
    "    image_a_embeddings = model.get_image_features(**image_a_inputs)\n",
    "    image_b_embeddings = model.get_image_features(**image_b_inputs)\n",
    "\n",
    "# Normalize the embeddings\n",
    "image_a_embeddings = image_a_embeddings / image_a_embeddings.norm(dim=-1, keepdim=True)\n",
    "image_b_embeddings = image_b_embeddings / image_b_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "similarity_score = torch.nn.functional.cosine_similarity(image_a_embeddings, image_b_embeddings)\n",
    "\n",
    "# Print the similarity score\n",
    "print(f\"CLIP Score: {similarity_score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP Score: 0.7435868978500366"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
